[
  {
    "id": 1,
    "name": "Xiaomi: MiMo-V2-Flash (free)",
    "usage_tokens": "533B tokens",
    "rankings": ["Academia (#4)", "Translation (#8)", "Finance (#9)"],
    "description": "MiMo-V2-Flash is an open-source foundation language model developed by Xiaomi. It is a Mixture-of-Experts model with 309B total parameters and 15B active parameters, adopting hybrid attention architecture. MiMo-V2-Flash supports a hybrid-thinking toggle and a 256K context window, and excels at reasoning, coding, and agent scenarios. On SWE-bench Verified and SWE-bench Multilingual, MiMo-V2-Flash ranks as the top #1 open-source model globally, delivering performance comparable to Claude Sonnet 4.5 while costing only about 3.5% as much. Users can control the reasoning behaviour with the reasoning enabled boolean. Learn more in our docs.",
    "author": "xiaomi",
    "context_window": "262K context",
    "capabilities": ["Reasoning", "Coding", "Agentic"]
  },
  {
    "id": 2,
    "name": "Mistral: Devstral 2 2512 (free)",
    "usage_tokens": "130B tokens",
    "rankings": ["Programming (#5)", "Legal (#7)", "SEO (#10)"],
    "description": "Devstral 2 is a state-of-the-art open-source model by Mistral AI specializing in agentic coding. It is a 123B-parameter dense transformer model supporting a 256K context window. Devstral 2 supports exploring codebases and orchestrating changes across multiple files while maintaining architecture-level context. It tracks framework dependencies, detects failures, and retries with corrections—solving challenges like bug fixing and modernizing legacy systems. The model can be fine-tuned to prioritize specific languages or optimize for large enterprise codebases. It is available under a modified MIT license.",
    "author": "mistralai",
    "context_window": "262K context",
    "capabilities": ["Agentic", "Coding", "Software Engineering"]
  },
  {
    "id": 3,
    "name": "TNG: DeepSeek R1T2 Chimera (free)",
    "usage_tokens": "93.2B tokens",
    "rankings": ["Roleplay (#5)"],
    "description": "DeepSeek-TNG-R1T2-Chimera is the second-generation Chimera model from TNG Tech. It is a 671 B-parameter mixture-of-experts text-generation model assembled from DeepSeek-AI’s R1-0528, R1, and V3-0324 checkpoints with an Assembly-of-Experts merge. The tri-parent design yields strong reasoning performance while running roughly 20 % faster than the original R1 and more than 2× faster than R1-0528 under vLLM, giving a favorable cost-to-intelligence trade-off. The checkpoint supports contexts up to 60 k tokens in standard use (tested to ~130 k) and maintains consistent <think> token behaviour, making it suitable for long-context analysis, dialogue and other open-ended generation tasks.",
    "author": "tngtech",
    "context_window": "164K context",
    "capabilities": ["Reasoning", "Long-Context", "Conversation"]
  },
  {
    "id": 4,
    "name": "TNG: DeepSeek R1T Chimera (free)",
    "usage_tokens": "23.2B tokens",
    "rankings": [],
    "description": "DeepSeek-R1T-Chimera is created by merging DeepSeek-R1 and DeepSeek-V3 (0324), combining the reasoning capabilities of R1 with the token efficiency improvements of V3. It is based on a DeepSeek-MoE Transformer architecture and is optimized for general text generation tasks. The model merges pretrained weights from both source models to balance performance across reasoning, efficiency, and instruction-following tasks. It is released under the MIT license and intended for research and commercial use.",
    "author": "tngtech",
    "context_window": "164K context",
    "capabilities": ["Reasoning", "Efficiency", "Instruction Following"]
  },
  {
    "id": 5,
    "name": "Z.AI: GLM 4.5 Air (free)",
    "usage_tokens": "23.1B tokens",
    "rankings": [],
    "description": "GLM-4.5-Air is the lightweight variant of our latest flagship model family, also purpose-built for agent-centric applications. Like GLM-4.5, it adopts the Mixture-of-Experts (MoE) architecture but with a more compact parameter size. GLM-4.5-Air also supports hybrid inference modes, offering a \"thinking mode\" for advanced reasoning and tool use, and a \"non-thinking mode\" for real-time interaction. Users can control the reasoning behaviour with the reasoning enabled boolean. Learn more in our docs",
    "author": "z-ai",
    "context_window": "131K context",
    "capabilities": ["Agentic", "Reasoning", "Tool Use"]
  },
  {
    "id": 6,
    "name": "DeepSeek: R1 0528 (free)",
    "usage_tokens": "13.5B tokens",
    "rankings": [],
    "description": "May 28th update to the original DeepSeek R1 Performance on par with OpenAI o1, but open-sourced and with fully open reasoning tokens. It's 671B parameters in size, with 37B active in an inference pass. Fully open-source model.",
    "author": "deepseek",
    "context_window": "164K context",
    "capabilities": ["Reasoning", "General Purpose"]
  },
  {
    "id": 7,
    "name": "Qwen: Qwen3 Coder 480B A35B (free)",
    "usage_tokens": "6.93B tokens",
    "rankings": [],
    "description": "Qwen3-Coder-480B-A35B-Instruct is a Mixture-of-Experts (MoE) code generation model developed by the Qwen team. It is optimized for agentic coding tasks such as function calling, tool use, and long-context reasoning over repositories. The model features 480 billion total parameters, with 35 billion active per forward pass (8 out of 160 experts). Pricing for the Alibaba endpoints varies by context length. Once a request is greater than 128k input tokens, the higher pricing is used.",
    "author": "qwen",
    "context_window": "262K context",
    "capabilities": ["Coding", "Agentic", "Tool Use", "Long-Context"]
  },
  {
    "id": 8,
    "name": "TNG: R1T Chimera (free)",
    "usage_tokens": "6.34B tokens",
    "rankings": [],
    "description": "TNG-R1T-Chimera is an experimental LLM with a faible for creative storytelling and character interaction. It is a derivate of the original TNG/DeepSeek-R1T-Chimera released in April 2025 and is available exclusively via Chutes and OpenRouter. Characteristics and improvements include: We think that it has a creative and pleasant personality. It has a preliminary EQ-Bench3 value of about 1305. It is quite a bit more intelligent than the original, albeit a slightly slower. It is much more think-token consistent, i.e. reasoning and answer blocks are properly delineated. Tool calling is much improved. TNG Tech, the model authors, ask that users follow the careful guidelines that Microsoft has created for their \"MAI-DS-R1\" DeepSeek-based model. These guidelines are available on Hugging Face (https://huggingface.co/microsoft/MAI-DS-R1).",
    "author": "tngtech",
    "context_window": "164K context",
    "capabilities": ["Creative Writing", "Roleplay", "Tool Use"]
  },
  {
    "id": 9,
    "name": "Meta: Llama 3.3 70B Instruct (free)",
    "usage_tokens": "3.48B tokens",
    "rankings": [],
    "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks. Supported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.",
    "author": "meta-llama",
    "context_window": "131K context",
    "capabilities": ["Multilingual", "Conversation"]
  },
  {
    "id": 10,
    "name": "Google: Gemma 3 27B (free)",
    "usage_tokens": "3.4B tokens",
    "rankings": [],
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 128k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs and function calling. Gemma 3 27B is Google's latest open source model, successor to Gemma 2",
    "author": "google",
    "context_window": "131K context",
    "capabilities": ["Multimodal", "Vision", "Math", "Reasoning", "Function Calling"]
  },
  {
    "id": 11,
    "name": "NVIDIA: Nemotron 3 Nano 30B A3B (free)",
    "usage_tokens": "3.33B tokens",
    "rankings": [],
    "description": "NVIDIA Nemotron 3 Nano 30B A3B is a small language MoE model with highest compute efficiency and accuracy for developers to build specialized agentic AI systems. The model is fully open with open-weights, datasets and recipes so developers can easily customize, optimize, and deploy the model on their infrastructure for maximum privacy and security.",
    "author": "nvidia",
    "context_window": "256K context",
    "capabilities": ["Agentic", "Specialized"]
  },
  {
    "id": 12,
    "name": "OpenAI: gpt-oss-120b (free)",
    "usage_tokens": "1.87B tokens",
    "rankings": [],
    "description": "gpt-oss-120b is an open-weight, 117B-parameter Mixture-of-Experts (MoE) language model from OpenAI designed for high-reasoning, agentic, and general-purpose production use cases. It activates 5.1B parameters per forward pass and is optimized to run on a single H100 GPU with native MXFP4 quantization. The model supports configurable reasoning depth, full chain-of-thought access, and native tool use.",
    "author": "openai",
    "context_window": "131K context",
    "capabilities": ["Reasoning", "Agentic", "General Purpose", "Tool Use"]
  },
  {
    "id": 13,
    "name": "Google: Gemini 2.0 Flash Experimental (free)",
    "usage_tokens": "1.24B tokens",
    "rankings": [],
    "description": "Gemini Flash 2.0 offers a significantly faster time to first token (TTFT) compared to Gemini Flash 1.5, while maintaining quality on par with larger models like Gemini Pro 1.5. It introduces notable enhancements in multimodal understanding, coding capabilities, complex instruction following, and function calling. Deprecating February 6, 2026",
    "author": "google",
    "context_window": "1.05M context",
    "capabilities": ["Multimodal", "Coding", "Instruction Following", "Function Calling", "Agentic"]
  },
  {
    "id": 14,
    "name": "Venice: Uncensored (free)",
    "usage_tokens": "1.1B tokens",
    "rankings": [],
    "description": "Venice Uncensored Dolphin Mistral 24B Venice Edition is a fine-tuned variant of Mistral-Small-24B-Instruct-2501, developed by dphn.ai in collaboration with Venice.ai. This model is designed as an “uncensored” instruct-tuned LLM, preserving user control over alignment, system prompts, and behavior. Intended for advanced and unrestricted use cases, Venice Uncensored emphasizes steerability and transparent behavior.",
    "author": "venice",
    "context_window": "33K context",
    "capabilities": ["Uncensored", "Steerability"]
  },
  {
    "id": 15,
    "name": "Nous: Hermes 3 405B Instruct (free)",
    "usage_tokens": "998M tokens",
    "rankings": [],
    "description": "Hermes 3 is a generalist language model with many improvements over Hermes 2, including advanced agentic capabilities, much better roleplaying, reasoning, multi-turn conversation, long context coherence, and improvements across the board. Hermes 3 405B is a frontier-level, full-parameter finetune of the Llama-3.1 405B foundation model.",
    "author": "nousresearch",
    "context_window": "131K context",
    "capabilities": ["Agentic", "Roleplay", "Reasoning", "Long-Context", "Function Calling", "Coding"]
  },
  {
    "id": 16,
    "name": "OpenAI: gpt-oss-20b (free)",
    "usage_tokens": "713M tokens",
    "rankings": [],
    "description": "gpt-oss-20b is an open-weight 21B parameter model released by OpenAI under the Apache 2.0 license. It uses a Mixture-of-Experts (MoE) architecture with 3.6B active parameters per forward pass, optimized for lower-latency inference and deployability on consumer or single-GPU hardware.",
    "author": "openai",
    "context_window": "131K context",
    "capabilities": ["Efficiency", "Reasoning", "Agentic", "Function Calling", "Tool Use"]
  },
  {
    "id": 17,
    "name": "NVIDIA: Nemotron Nano 12B 2 VL (free)",
    "usage_tokens": "599M tokens",
    "rankings": [],
    "description": "NVIDIA Nemotron Nano 2 VL is a 12-billion-parameter open multimodal reasoning model designed for video understanding and document intelligence. It introduces a hybrid Transformer-Mamba architecture, combining transformer-level accuracy with Mamba’s memory-efficient sequence modeling for significantly higher throughput and lower latency.",
    "author": "nvidia",
    "context_window": "128K context",
    "capabilities": ["Video", "Vision", "OCR", "Document Intelligence"]
  },
  {
    "id": 18,
    "name": "Mistral: Mistral Small 3.1 24B (free)",
    "usage_tokens": "421M tokens",
    "rankings": [],
    "description": "Mistral Small 3.1 24B Instruct is an upgraded variant of Mistral Small 3 (2501), featuring 24 billion parameters with advanced multimodal capabilities. It provides state-of-the-art performance in text-based reasoning and vision tasks, including image analysis, programming, mathematical reasoning, and multilingual support.",
    "author": "mistralai",
    "context_window": "128K context",
    "capabilities": ["Multimodal", "Vision", "Reasoning", "Coding", "Math", "Multilingual", "Function Calling"]
  },
  {
    "id": 19,
    "name": "Arcee AI: Trinity Mini (free)",
    "usage_tokens": "415M tokens",
    "rankings": [],
    "description": "Trinity Mini is a 26B-parameter (3B active) sparse mixture-of-experts language model featuring 128 experts with 8 active per token. Engineered for efficient reasoning over long contexts (131k) with robust function calling and multi-step agent workflows.",
    "author": "arcee-ai",
    "context_window": "131K context",
    "capabilities": ["Reasoning", "Long-Context", "Function Calling", "Agentic"]
  },
  {
    "id": 20,
    "name": "AllenAI: Molmo2 8B (free)",
    "usage_tokens": "340M tokens",
    "rankings": [],
    "description": "Molmo2-8B is an open vision-language model developed by the Allen Institute for AI (Ai2) as part of the Molmo2 family, supporting image, video, and multi-image understanding and grounding. It is based on Qwen3-8B and uses SigLIP 2 as its vision backbone.",
    "author": "allenai",
    "context_window": "37K context",
    "capabilities": ["Multimodal", "Vision", "Video"]
  },
  {
    "id": 21,
    "name": "NVIDIA: Nemotron Nano 9B V2 (free)",
    "usage_tokens": "298M tokens",
    "rankings": [],
    "description": "NVIDIA-Nemotron-Nano-9B-v2 is a large language model (LLM) trained from scratch by NVIDIA, and designed as a unified model for both reasoning and non-reasoning tasks. It responds to user queries and tasks by first generating a reasoning trace and then concluding with a final response.",
    "author": "nvidia",
    "context_window": "128K context",
    "capabilities": ["Reasoning", "General Purpose"]
  },
  {
    "id": 22,
    "name": "Qwen: Qwen2.5-VL 7B Instruct (free)",
    "usage_tokens": "224M tokens",
    "rankings": [],
    "description": "Qwen2.5 VL 7B is a multimodal LLM from the Qwen Team with enhancements in visual understanding, video comprehension of 20min+, and agent capabilities for mobiles and robots. Supports multilingual text within images.",
    "author": "qwen",
    "context_window": "33K context",
    "capabilities": ["Vision", "Video", "Agentic", "Multilingual"]
  },
  {
    "id": 23,
    "name": "Meta: Llama 3.2 3B Instruct (free)",
    "usage_tokens": "137M tokens",
    "rankings": [],
    "description": "Llama 3.2 3B is a 3-billion-parameter multilingual large language model, optimized for advanced natural language processing tasks like dialogue generation, reasoning, and summarization. Trained on 9 trillion tokens.",
    "author": "meta-llama",
    "context_window": "131K context",
    "capabilities": ["Conversation", "Reasoning", "Instruction Following", "Tool Use"]
  },
  {
    "id": 24,
    "name": "Qwen: Qwen3 4B (free)",
    "usage_tokens": "130M tokens",
    "rankings": [],
    "description": "Qwen3-4B is a 4 billion parameter dense language model from the Qwen3 series, designed to support both general-purpose and reasoning-intensive tasks with a dual-mode thinking architecture.",
    "author": "qwen",
    "context_window": "41K context",
    "capabilities": ["Reasoning", "Instruction Following", "Agentic"]
  },
  {
    "id": 25,
    "name": "Qwen: Qwen3 Next 80B A3B Instruct (free)",
    "usage_tokens": "69.4M tokens",
    "rankings": [],
    "description": "Qwen3-Next-80B-A3B-Instruct is an instruction-tuned chat model optimized for fast, stable responses without thinking traces. Ideal for RAG, tool use, and agentic workflows requiring consistent answers.",
    "author": "qwen",
    "context_window": "262K context",
    "capabilities": ["Reasoning", "Coding", "Multilingual", "RAG", "Tool Use", "Agentic"]
  },
  {
    "id": 26,
    "name": "Google: Gemma 3 4B (free)",
    "usage_tokens": "52.3M tokens",
    "rankings": [],
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. Offers improved math, reasoning, and chat capabilities, including structured outputs and function calling.",
    "author": "google",
    "context_window": "33K context",
    "capabilities": ["Multimodal", "Math", "Reasoning", "Conversation", "Function Calling"]
  },
  {
    "id": 27,
    "name": "Google: Gemma 3n 2B (free)",
    "usage_tokens": "45.3M tokens",
    "rankings": [],
    "description": "Gemma 3n E2B IT is a multimodal, instruction-tuned model developed by Google DeepMind, designed to operate efficiently at an effective parameter size of 2B while leveraging a 6B architecture.",
    "author": "google",
    "context_window": "8K context",
    "capabilities": ["Multimodal", "Instruction Following", "Multilingual", "Reasoning"]
  },
  {
    "id": 28,
    "name": "Google: Gemma 3 12B (free)",
    "usage_tokens": "42M tokens",
    "rankings": [],
    "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. Handles context windows up to 128k tokens and understands over 140 languages.",
    "author": "google",
    "context_window": "33K context",
    "capabilities": ["Multimodal", "Math", "Reasoning", "Conversation", "Function Calling"]
  },
  {
    "id": 29,
    "name": "Google: Gemma 3n 4B (free)",
    "usage_tokens": "20.2M tokens",
    "rankings": [],
    "description": "Gemma 3n E4B-it is optimized for efficient execution on mobile and low-resource devices. Supports multimodal inputs including text, visual data, and audio.",
    "author": "google",
    "context_window": "8K context",
    "capabilities": ["Multimodal", "Speech", "Translation", "Vision"]
  },
  {
    "id": 30,
    "name": "MoonshotAI: Kimi K2 0711 (free)",
    "usage_tokens": "N/A",
    "rankings": [],
    "description": "Kimi K2 Instruct is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. Optimized for agentic capabilities.",
    "author": "moonshotai",
    "context_window": "33K context",
    "capabilities": ["Agentic", "Tool Use", "Reasoning", "Coding"]
  },
  {
    "id": 31,
    "name": "Meta: Llama 3.1 405B Instruct (free)",
    "usage_tokens": "N/A",
    "rankings": [],
    "description": "Meta's latest class of model (Llama 3.1) launched with a variety of sizes & flavors. This 405B instruct-tuned version is optimized for high quality dialogue usecases. Demonstated strong performance compared to GPT-4o.",
    "author": "meta-llama",
    "context_window": "131K context",
    "capabilities": ["Conversation", "Reasoning", "General Purpose"]
  }
]
